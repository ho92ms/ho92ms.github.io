---
title: "Research"
permalink: /research/
layout: single
---

## Research Interests

My research interests lie at the intersection of **applied machine learning** and **system-level engineering**, with a focus on methods that are both **theoretically grounded** and **practically useful**.

---

## Core Topics

### 1. Representation Learning
- **Neural network architectures** — understanding and designing effective representations.
- **Latent space structure** — interpretable embeddings and manifold learning.
- **Transfer learning** — leveraging pre-trained models for domain-specific tasks.

### 2. Large Language Models (LLMs)
- **Prompt engineering** and optimization strategies.
- **Fine-tuning techniques** for specialized domains.
- **Interpretability** — understanding model behavior and decision-making processes.
- **Practical applications** — building LLM-based systems with reliability and transparency.

### 3. Interpretable Machine Learning
- **Structurally transparent models** — preferring architectures with clear inductive biases.
- **Explainability methods** — post-hoc analysis and intrinsic interpretability.
- **Model auditing** — validation, fairness, and robustness guarantees.

### 4. System-Level Performance
- **Scalability** — designing ML systems that handle large-scale data efficiently.
- **Deployment** — bridging the gap between research prototypes and production systems.
- **Optimization** — balancing model complexity with computational constraints.

---

## Methodological Principles

I emphasize approaches that prioritize:

1. **Structure over ad-hoc optimization** — leveraging domain knowledge and formal priors.
2. **Reproducibility** — transparent methodologies and open-source implementations.
3. **Empirical validation** — rigorous experimentation with well-defined metrics.

---

## Current Focus

My current work explores:

- **LLM-based agents** — designing systems with clear reasoning and error-handling mechanisms.
- **Neural architecture design** — exploring inductive biases that improve generalization.
- **Data efficiency** — methods that achieve strong performance with limited labeled data.

---

## Publications & Projects

Selected work and experiments are available on my [GitHub](https://github.com/ho92ms).

For collaboration or discussions, feel free to reach out at [neduabi@pm.me](mailto:neduabi@pm.me).

---

> *"Structure, interpretability, and reproducibility over ad-hoc optimization."*


